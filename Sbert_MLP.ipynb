{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sbert_MLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "n_iPx_BJLcmR",
        "outputId": "2e38b39a-e04b-458a-e62e-d523300c09fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParseError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m no element found: line 1, column 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zouVDhkGCXiR",
        "outputId": "b9c13405-3759-4264-9740-b0ca1d9b844c"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "!pip install -U sentence-transformers\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "from pathlib import Path\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import reduce\n",
        "from math import log\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "import json\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "import pickle\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "from sklearn.model_selection import KFold\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 29.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers>=0.10.3\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 60.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 77.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=aa9f62252d6166d25a3ffee00cbd401c5b9cf04f55d6af130fb151f278da30c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNSJ-vqeYSEq"
      },
      "source": [
        "dir=\"/content/drive/MyDrive/nlp_project/\"\n",
        "os.chdir(dir)\n",
        "df=pd.read_csv(\"train.csv\")\n",
        "output=list(df[\"is_duplicate\"])\n",
        "output_np=np.array(output)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJV1CmCvPbSp"
      },
      "source": [
        "\n",
        "output_encode=to_categorical(output_np)\n",
        "output_encode=np.array(output_encode)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAKLoILdCE6D"
      },
      "source": [
        "open_file = open(\"embeddings_para_1\", \"rb\")\n",
        "embeddings1= pickle.load(open_file)\n",
        "open_file.close()\n",
        "open_file = open(\"embeddings_para_2\", \"rb\")\n",
        "embeddings2 = pickle.load(open_file)\n",
        "open_file.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2OIKoamXbfO"
      },
      "source": [
        "embeddings1=np.array(embeddings1)\n",
        "embeddings2=np.array(embeddings2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-y2A1hmXAkJ"
      },
      "source": [
        "input=np.hstack((embeddings1,embeddings2))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biMM29AyXV7T",
        "outputId": "709a2edb-cfdd-4ac8-86cb-bda1f97cfe18"
      },
      "source": [
        "print(input.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404348, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb02Nl2A7SH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecdd58b3-fb28-4f3e-a20f-6f35569b1af1"
      },
      "source": [
        "accuracy=[]\n",
        "loss=[]\n",
        "predictions=[]\n",
        "truth=[]\n",
        "X_train, X_test, y_train, y_test = train_test_split(input, output_encode, test_size=0.25, random_state=42)\n",
        "model = Sequential()\n",
        "#model.add(Dense(1024, input_dim=768,activation='relu'))\n",
        "model.add(Dense(512, input_dim=768,activation='relu'))\n",
        "#model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc'])\n",
        "model.fit(X_train,y_train,epochs=6,validation_split=0.2,batch_size=64)\n",
        "score=model.evaluate(X_test,y_test)\n",
        "print(score[1])\n",
        "accuracy.append(score[1])\n",
        "loss.append(score[0])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred= np.argmax(y_pred,axis=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               393728    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 567,282\n",
            "Trainable params: 567,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/6\n",
            "3791/3791 [==============================] - 29s 7ms/step - loss: 0.4503 - acc: 0.7748 - val_loss: 0.3938 - val_acc: 0.8095\n",
            "Epoch 2/6\n",
            "3791/3791 [==============================] - 28s 7ms/step - loss: 0.3557 - acc: 0.8299 - val_loss: 0.3776 - val_acc: 0.8176\n",
            "Epoch 3/6\n",
            "3791/3791 [==============================] - 28s 7ms/step - loss: 0.3069 - acc: 0.8573 - val_loss: 0.4050 - val_acc: 0.8176\n",
            "Epoch 4/6\n",
            "3791/3791 [==============================] - 28s 7ms/step - loss: 0.2658 - acc: 0.8793 - val_loss: 0.4017 - val_acc: 0.8220\n",
            "Epoch 5/6\n",
            "3791/3791 [==============================] - 28s 7ms/step - loss: 0.2294 - acc: 0.8984 - val_loss: 0.4198 - val_acc: 0.8195\n",
            "Epoch 6/6\n",
            "3791/3791 [==============================] - 28s 7ms/step - loss: 0.1993 - acc: 0.9136 - val_loss: 0.4724 - val_acc: 0.8228\n",
            "3159/3159 [==============================] - 8s 2ms/step - loss: 0.4709 - acc: 0.8239\n",
            "0.8238744735717773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTPWYOJY6txE",
        "outputId": "56aa360b-4ce2-4f46-c416-2e895ad6c0a9"
      },
      "source": [
        "y_test=np.argmax(y_test,axis=1)\n",
        "model.save(\"model_deploy\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_deploy/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELnm32PLQNep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f388502e-b2fa-4929-ede8-358ea7ad5d48"
      },
      "source": [
        "print(\"S_Bert+MLP\")\n",
        "print(\"Accuracy: \",accuracy_score(y_test, y_pred)*100)\n",
        "print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"F1 Score:\\n \",f1_score(y_test, y_pred)*100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S_Bert+MLP\n",
            "Accuracy:  82.38744843550604\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86     64078\n",
            "           1       0.76      0.76      0.76     37009\n",
            "\n",
            "    accuracy                           0.82    101087\n",
            "   macro avg       0.81      0.81      0.81    101087\n",
            "weighted avg       0.82      0.82      0.82    101087\n",
            "\n",
            "F1 Score:\n",
            "  75.98726801899008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9dJvJ7I6ptB",
        "outputId": "2b0e4f00-8e6d-4b67-f314-6586892ab384"
      },
      "source": [
        "i=X_train[2]\n",
        "i=i.reshape(1,-1)\n",
        "i.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnLC-YVFOxIr"
      },
      "source": [
        "y_pred = model.predict(i)\n",
        "y_pred= np.argmax(y_pred,axis=1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7NM6E7VO9d5",
        "outputId": "66c6f2e0-7b6c-409f-d090-1f82ac715478"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btWmGRO6PQN5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
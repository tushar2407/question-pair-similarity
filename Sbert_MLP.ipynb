{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sbert_MLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zouVDhkGCXiR",
        "outputId": "289eef7a-a1ae-42d5-de57-eb469bde0c46"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "!pip install -U sentence-transformers\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "from pathlib import Path\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import reduce\n",
        "from math import log\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "\n",
        "import json\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "import pickle\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "t = TweetTokenizer()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.12.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNSJ-vqeYSEq"
      },
      "source": [
        "dir=\"/content/drive/MyDrive/nlp_project/\"\n",
        "os.chdir(dir)\n",
        "df=pd.read_csv(\"train.csv\")\n",
        "output=list(df[\"is_duplicate\"])\n",
        "output_np=np.array(output)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJV1CmCvPbSp"
      },
      "source": [
        "\n",
        "output_encode=to_categorical(output_np)\n",
        "output_encode=np.array(output_encode)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or5uy3SSYXBS",
        "outputId": "6c8bfa8e-b082-4316-cc75-ddeceebc324e"
      },
      "source": [
        "print(output_encode[:40])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAKLoILdCE6D"
      },
      "source": [
        "open_file = open(\"embeddings_para_1\", \"rb\")\n",
        "embeddings1= pickle.load(open_file)\n",
        "open_file.close()\n",
        "open_file = open(\"embeddings_para_2\", \"rb\")\n",
        "embeddings2 = pickle.load(open_file)\n",
        "open_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2OIKoamXbfO"
      },
      "source": [
        "embeddings1=np.array(embeddings1)\n",
        "embeddings2=np.array(embeddings2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-y2A1hmXAkJ"
      },
      "source": [
        "input=np.hstack((embeddings1,embeddings2))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biMM29AyXV7T",
        "outputId": "bcf46fc3-c91e-4a27-dc53-f3b719b81420"
      },
      "source": [
        "print(input.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404348, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb02Nl2A7SH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dbd7601-e671-4f83-d1e3-b183329024ab"
      },
      "source": [
        "accuracy=[]\n",
        "loss=[]\n",
        "predictions=[]\n",
        "truth=[]\n",
        "X_train, X_test, y_train, y_test = train_test_split(input, output_encode, test_size=0.25, random_state=42)\n",
        "model = Sequential()\n",
        "#model.add(Dense(1024, input_dim=768,activation='relu'))\n",
        "model.add(Dense(512, input_dim=768,activation='relu'))\n",
        "#model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc'])\n",
        "model.fit(X_train,y_train,epochs=6,validation_split=0.2,batch_size=64)\n",
        "score=model.evaluate(X_test,y_test)\n",
        "print(score[1])\n",
        "accuracy.append(score[1])\n",
        "loss.append(score[0])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred= np.argmax(y_pred,axis=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               393728    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 567,282\n",
            "Trainable params: 567,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/6\n",
            "3791/3791 [==============================] - 35s 9ms/step - loss: 0.4488 - acc: 0.7760 - val_loss: 0.3981 - val_acc: 0.8058\n",
            "Epoch 2/6\n",
            "3791/3791 [==============================] - 35s 9ms/step - loss: 0.3565 - acc: 0.8302 - val_loss: 0.3865 - val_acc: 0.8153\n",
            "Epoch 3/6\n",
            "3791/3791 [==============================] - 35s 9ms/step - loss: 0.3073 - acc: 0.8571 - val_loss: 0.3898 - val_acc: 0.8201\n",
            "Epoch 4/6\n",
            "3791/3791 [==============================] - 35s 9ms/step - loss: 0.2650 - acc: 0.8801 - val_loss: 0.3926 - val_acc: 0.8206\n",
            "Epoch 5/6\n",
            "3791/3791 [==============================] - 36s 10ms/step - loss: 0.2297 - acc: 0.8983 - val_loss: 0.4379 - val_acc: 0.8221\n",
            "Epoch 6/6\n",
            "3791/3791 [==============================] - 34s 9ms/step - loss: 0.1978 - acc: 0.9147 - val_loss: 0.4748 - val_acc: 0.8216\n",
            "3159/3159 [==============================] - 9s 3ms/step - loss: 0.4628 - acc: 0.8244\n",
            "0.8243592381477356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTPWYOJY6txE"
      },
      "source": [
        "y_test=np.argmax(y_test,axis=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELnm32PLQNep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a3cab9-16b2-4a8f-f7ee-53238d69eebd"
      },
      "source": [
        "print(\"S_Bert+MLP\")\n",
        "print(\"Accuracy: \",accuracy_score(y_test, y_pred)*100)\n",
        "print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"F1 Score:\\n \",f1_score(y_test, y_pred)*100)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S_Bert+MLP\n",
            "Accuracy:  82.43592153293697\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86     64078\n",
            "           1       0.76      0.76      0.76     37009\n",
            "\n",
            "    accuracy                           0.82    101087\n",
            "   macro avg       0.81      0.81      0.81    101087\n",
            "weighted avg       0.82      0.82      0.82    101087\n",
            "\n",
            "F1 Score:\n",
            "  75.91726008816548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9dJvJ7I6ptB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "import keras.layers as lyr\n",
    "from keras.models import Model, load_model\n",
    "import pickle\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './dataset/'\n",
    "train = pd.read_csv(f'{BASE_DIR}train.csv')\n",
    "test = pd.read_csv(f'{BASE_DIR}test.csv')\n",
    "# train = pd.read_csv(f'{BASE_DIR}train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id'] = train['id'].apply(str)\n",
    "test['test_id'] = test['test_id'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((train, test))\n",
    "# df = train\n",
    "df['question1'].fillna('', inplace=True)\n",
    "df['question2'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_vectorizer = CountVectorizer(max_features=10000-1).fit(\n",
    "#     itertools.chain(\n",
    "#         df['question1'], \n",
    "#         df['question2']\n",
    "#         )\n",
    "#     )\n",
    "counts_vectorizer = pickle.load(open(\"./models/analysis2/bow.pkl\", \"rb\"))\n",
    "other_index = len(counts_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(counts_vectorizer, open(\"./models/analysis2/bow.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenizer = re.compile(counts_vectorizer.token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_seqs(texts, max_len=10):\n",
    "    seqs = texts.apply(\n",
    "        lambda s: \n",
    "            [\n",
    "                counts_vectorizer.vocabulary_[w] if w in counts_vectorizer.vocabulary_ else other_index\n",
    "                for w in words_tokenizer.findall(s.lower())\n",
    "            ]\n",
    "        )\n",
    "    return pad_sequences(seqs, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        create_padded_seqs(df[df['id'].notnull()]['question1']), \n",
    "        create_padded_seqs(df[df['id'].notnull()]['question2']),\n",
    "        df[df['id'].notnull()]['is_duplicate'].values,\n",
    "        stratify=df[df['id'].notnull()]['is_duplicate'].values,\n",
    "        test_size=0.3, random_state=1989\n",
    "    )\n",
    "X1_val, X1_test, X2_val, X2_test, y_val, y_test = \\\n",
    "    train_test_split(\n",
    "        X1_val, \n",
    "        X2_val,\n",
    "        y_val,\n",
    "        stratify=y_val,\n",
    "        test_size=0.5, random_state=1989\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 10, 100)      1000000     ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 256)          365568      ['embedding_1[0][0]',            \n",
      "                                                                  'embedding_1[1][0]']            \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 256)          0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm_1[1][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           4112        ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            17          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,369,697\n",
      "Trainable params: 1,369,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1_tensor = lyr.Input(X1_train.shape[1:])\n",
    "input2_tensor = lyr.Input(X2_train.shape[1:])\n",
    "\n",
    "words_embedding_layer = lyr.Embedding(X1_train.max() + 1, 100)\n",
    "seq_embedding_layer = lyr.LSTM(256, activation='tanh')\n",
    "\n",
    "seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))\n",
    "\n",
    "merge_layer = lyr.multiply([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])\n",
    "\n",
    "dense1_layer = lyr.Dense(16, activation='sigmoid')(merge_layer)\n",
    "ouput_layer = lyr.Dense(1, activation='sigmoid')(dense1_layer)\n",
    "\n",
    "model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./models/analysis2/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2211/2211 - 354s - loss: 0.5196 - val_loss: 0.4758 - 354s/epoch - 160ms/step\n",
      "Epoch 2/6\n",
      "2211/2211 - 318s - loss: 0.4430 - val_loss: 0.4468 - 318s/epoch - 144ms/step\n",
      "Epoch 3/6\n",
      "2211/2211 - 378s - loss: 0.3918 - val_loss: 0.4338 - 378s/epoch - 171ms/step\n",
      "Epoch 4/6\n",
      "2211/2211 - 319s - loss: 0.3466 - val_loss: 0.4277 - 319s/epoch - 144ms/step\n",
      "Epoch 5/6\n",
      "2211/2211 - 326s - loss: 0.3058 - val_loss: 0.4304 - 326s/epoch - 147ms/step\n",
      "Epoch 6/6\n",
      "2211/2211 - 309s - loss: 0.2663 - val_loss: 0.4469 - 309s/epoch - 140ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b2be909580>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit([X1_train, X2_train], y_train, \n",
    "#           validation_data=([X1_val, X2_val], y_val), \n",
    "#           batch_size=128, epochs=6, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"./models/analysis2/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X1_test, X2_test],128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score :  0.8106325440274388\n",
      "F1 Score :  0.7459741638648026\n",
      "Precision :  0.7389779998246998\n",
      "Recall :  0.7531040643144261\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score : \", f1_score(y_test, y_pred))\n",
    "print(\"Precision : \", precision_score(y_test, y_pred))\n",
    "print(\"Recall : \", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_model = Model([input1_tensor, input2_tensor], merge_layer)\n",
    "# features_model.compile(loss='mse', optimizer='adam')\n",
    "# F_train = features_model.predict([X1_train, X2_train], batch_size=128)\n",
    "# F_val = features_model.predict([X1_val, X2_val], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(merge_layer, open(\"./models/analysis2/merge_layer_unprocessed.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48cb8e7cdb11ca655815c41ac34bed6720108475fb3aac9b379c032798fb60bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

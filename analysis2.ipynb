{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras.layers as lyr\n",
    "from keras.models import Model\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './dataset/'\n",
    "train = pd.read_csv(f'{BASE_DIR}train.csv')\n",
    "test = pd.read_csv(f'{BASE_DIR}test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id'] = train['id'].apply(str)\n",
    "test['test_id'] = test['test_id'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((train, test))\n",
    "df['question1'].fillna('', inplace=True)\n",
    "df['question2'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_vectorizer = CountVectorizer(max_features=10000-1).fit(\n",
    "    itertools.chain(\n",
    "        df['question1'], \n",
    "        df['question2']\n",
    "        )\n",
    "    )\n",
    "other_index = len(counts_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenizer = re.compile(counts_vectorizer.token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_seqs(texts, max_len=10):\n",
    "    seqs = texts.apply(\n",
    "        lambda s: \n",
    "            [\n",
    "                counts_vectorizer.vocabulary_[w] if w in counts_vectorizer.vocabulary_ else other_index\n",
    "                for w in words_tokenizer.findall(s.lower())\n",
    "            ]\n",
    "        )\n",
    "    return pad_sequences(seqs, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        create_padded_seqs(df[df['id'].notnull()]['question1']), \n",
    "        create_padded_seqs(df[df['id'].notnull()]['question2']),\n",
    "        df[df['id'].notnull()]['is_duplicate'].values,\n",
    "        stratify=df[df['id'].notnull()]['is_duplicate'].values,\n",
    "        test_size=0.3, random_state=1989\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 10, 100)      1000000     ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          365568      ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 256)          0           ['lstm[0][0]',                   \n",
      "                                                                  'lstm[1][0]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           4112        ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            17          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,369,697\n",
      "Trainable params: 1,369,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1_tensor = lyr.Input(X1_train.shape[1:])\n",
    "input2_tensor = lyr.Input(X2_train.shape[1:])\n",
    "\n",
    "words_embedding_layer = lyr.Embedding(X1_train.max() + 1, 100)\n",
    "seq_embedding_layer = lyr.LSTM(256, activation='tanh')\n",
    "\n",
    "seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))\n",
    "\n",
    "merge_layer = lyr.multiply([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])\n",
    "\n",
    "dense1_layer = lyr.Dense(16, activation='sigmoid')(merge_layer)\n",
    "ouput_layer = lyr.Dense(1, activation='sigmoid')(dense1_layer)\n",
    "\n",
    "model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2211/2211 - 336s - loss: 0.5174 - val_loss: 0.4728 - 336s/epoch - 152ms/step\n",
      "Epoch 2/6\n",
      "2211/2211 - 316s - loss: 0.4375 - val_loss: 0.4426 - 316s/epoch - 143ms/step\n",
      "Epoch 3/6\n",
      "2211/2211 - 270s - loss: 0.3874 - val_loss: 0.4347 - 270s/epoch - 122ms/step\n",
      "Epoch 4/6\n",
      "2211/2211 - 304s - loss: 0.3437 - val_loss: 0.4295 - 304s/epoch - 137ms/step\n",
      "Epoch 5/6\n",
      "2211/2211 - 315s - loss: 0.3019 - val_loss: 0.4397 - 315s/epoch - 143ms/step\n",
      "Epoch 6/6\n",
      "2211/2211 - 289s - loss: 0.2625 - val_loss: 0.4547 - 289s/epoch - 131ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2dc83f6fb80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X1_train, X2_train], y_train, \n",
    "          validation_data=([X1_val, X2_val], y_val), \n",
    "          batch_size=128, epochs=6, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing the neural network\n",
    "# # df = pd.read_csv(f'{BASE_DIR}test_1.csv')\n",
    "# # df['test_id'] = df['test_id'].apply(str)\n",
    "# # df.fillna('', inplace=True)\n",
    "\n",
    "# _, X1_test, _, X2_test, _, y_test = \\\n",
    "#     train_test_split(\n",
    "#         create_padded_seqs(df[df['test_id'].notnull()]['question1']), \n",
    "#         create_padded_seqs(df[df['test_id'].notnull()]['question2']),\n",
    "#         df[df['test_id'].notnull()]['is_duplicate'].values,\n",
    "#         stratify=df[df['test_id'].notnull()]['is_duplicate'].values,\n",
    "#         test_size=0.999, random_state=1989\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.predict([X1_test, X2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_model = Model([input1_tensor, input2_tensor], merge_layer)\n",
    "features_model.compile(loss='mse', optimizer='adam')\n",
    "F_train = features_model.predict([X1_train, X2_train], batch_size=128)\n",
    "F_val = features_model.predict([X1_val, X2_val], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XgBoost on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTrain = xgb.DMatrix(F_train, label=y_train)\n",
    "dVal = xgb.DMatrix(F_val, label=y_val)\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1, \n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n",
    "    'min_child_weight': 5,\n",
    "    'silent': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:05:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:0.65421\tval-logloss:0.65995\n",
      "[10]\ttrain-logloss:0.44260\tval-logloss:0.49366\n",
      "[20]\ttrain-logloss:0.34823\tval-logloss:0.44103\n",
      "[30]\ttrain-logloss:0.30364\tval-logloss:0.42468\n",
      "[40]\ttrain-logloss:0.27322\tval-logloss:0.41931\n",
      "[49]\ttrain-logloss:0.25369\tval-logloss:0.41959\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dVal,'val')], \n",
    "                verbose_eval=10, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = create_padded_seqs(df[df['test_id'].notnull()]['question1'])\n",
    "X2_test = create_padded_seqs(df[df['test_id'].notnull()]['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_test = features_model.predict([X1_test, X2_test], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTest = xgb.DMatrix(F_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "        'test_id': df[df['test_id'].notnull()]['test_id'].values,\n",
    "        'is_duplicate': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n",
    "    }).set_index('test_id')\n",
    "\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['is_duplicate'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48cb8e7cdb11ca655815c41ac34bed6720108475fb3aac9b379c032798fb60bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

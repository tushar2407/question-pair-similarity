{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "import scipy #library for scientific calculations\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "from sklearn import pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = './dataset/'\n",
    "train = pd.read_csv(f'{BASE_DIR}train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255027\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_str(s):\n",
    "    return ' '+s+' '\n",
    "## cleaning the questions\n",
    "def normalize_text(text):\n",
    "    SPECIAL_TOKENS = {'non-ascii': 'non_ascii_word'}\n",
    "\n",
    "    if pd.isnull(text) or len(text)==0:\n",
    "        return ''\n",
    "\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "\n",
    "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) \n",
    "\n",
    "    #Removing Punctuations\n",
    "    text = [word for word in text if word not in punctuation]\n",
    "    text = ''.join(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Return a list of words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what is the story of kohinoor kohinoor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math2324math is divide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  what is the step by step guide to invest in sh...   \n",
       "1   1     3     4     what is the story of kohinoor kohinoor diamond   \n",
       "2   2     5     6  how can i increase the speed of my internet co...   \n",
       "3   3     7     8   why am i mentally very lonely how can i solve it   \n",
       "4   4     9    10  which one dissolve in water quikly sugar salt ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  what is the step by step guide to invest in sh...             0  \n",
       "1  what would happen if the indian government sto...             0  \n",
       "2  how can internet speed be increased by hacking...             0  \n",
       "3  find the remainder when math2324math is divide...             0  \n",
       "4             which fish would survive in salt water             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['question1'] = train['question1'].apply(normalize_text)\n",
    "train['question2'] = train['question2'].apply(normalize_text)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('./processed_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words + XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using word vector of word_count and frequency withpout capturing the meaning of word\n",
    "#r'\\w{1,}' indiactes 1 or more word\n",
    "\n",
    "CV = CountVectorizer(analyzer='word', stop_words='english', token_pattern=r'\\w{1,}')\n",
    "q1_trans = CV.fit_transform(train['question1'].values)\n",
    "q2_trans = CV.fit_transform(train['question2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy.sparse.hstack will stack sparse matrix columnwise, and stacking them side by side\n",
    "\n",
    "X = scipy.sparse.hstack((q1_trans, q2_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\Sem_5\\NLP\\Project\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:01:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, eta=0.3, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=1, subsample=0.8, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "Confusion Matrix:\n",
      " [[71408  4942]\n",
      " [25489 19448]]\n",
      "Accuracy score: \n",
      " 0.7490992439420548\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.94      0.82     76350\n",
      "           1       0.80      0.43      0.56     44937\n",
      "\n",
      "    accuracy                           0.75    121287\n",
      "   macro avg       0.77      0.68      0.69    121287\n",
      "weighted avg       0.76      0.75      0.73    121287\n",
      "\n",
      "F1 Score:\n",
      "  0.5610512498737865\n",
      "Code run-time:  0:01:49.976266\n"
     ]
    }
   ],
   "source": [
    "#gradient Boosting Model used\n",
    "#start time\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "classifier1 = XGBClassifier(\n",
    "    max_depth=50, \n",
    "    n_estimators=80, \n",
    "    learning_rate=0.1, \n",
    "    colsample_bytree=.7, \n",
    "    gamma=0, \n",
    "    reg_alpha=4, \n",
    "    objective='binary:logistic', \n",
    "    eta=0.3, \n",
    "    silent=1, \n",
    "    subsample=0.8,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "#fitting the model\n",
    "print(classifier1.fit(X_train, y_train))\n",
    "#predicting if pair is duplicate or not\n",
    "prediction_CV = classifier1.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_CV))\n",
    "print(\"Accuracy score: \\n\", accuracy_score(y_test, prediction_CV))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, prediction_CV))\n",
    "print(\"F1 Score:\\n \",f1_score(y_test, prediction_CV))\n",
    "\n",
    "et = datetime.datetime.now()\n",
    "print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF (word level) + XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 features were used for tfidf vectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', max_features=5000, token_pattern=r'\\w{1,}')\n",
    "\n",
    "q1word_trans = tfidf.fit_transform(train['question1'].values)\n",
    "q2word_trans = tfidf.fit_transform(train['question2'].values)\n",
    "\n",
    "X = scipy.sparse.hstack((q1word_trans,q2word_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:08:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, eta=0.3, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=1, subsample=0.8, tree_method='exact',\n",
      "              use_label_encoder=False, validate_parameters=1, ...)\n",
      "Confusion Matrix:\n",
      " [[75553  8422]\n",
      " [19905 29536]]\n",
      "Accuracy score: \n",
      " 0.787679138933861\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84     83975\n",
      "           1       0.78      0.60      0.68     49441\n",
      "\n",
      "    accuracy                           0.79    133416\n",
      "   macro avg       0.78      0.75      0.76    133416\n",
      "weighted avg       0.79      0.79      0.78    133416\n",
      "\n",
      "F1 Score:\n",
      "  0.6758887401457683\n",
      "Code run-time:  0:08:04.506296\n"
     ]
    }
   ],
   "source": [
    "# Xg Boost classifier for word level vectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "classifier2 = XGBClassifier(\n",
    "    max_depth=50, \n",
    "    n_estimators=80, \n",
    "    learning_rate=0.1, \n",
    "    colsample_bytree=.7, \n",
    "    gamma=0, reg_alpha=4, \n",
    "    objective='binary:logistic', \n",
    "    eta=0.3, \n",
    "    silent=1, \n",
    "    subsample=0.8,\n",
    "    use_label_encoder=False)\n",
    "\n",
    "#fitting the model with traing data\n",
    "print(classifier2.fit(X_train, y_train))\n",
    "\n",
    "#predicting the test data\n",
    "prediction_tfidf = classifier2.predict(X_test)\n",
    "\n",
    "#Performance evaluation\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\n",
    "print(\"Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, prediction_tfidf))\n",
    "print(\"F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n",
    "\n",
    "et = datetime.datetime.now()\n",
    "print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf (ngram level) + XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF ngram level vectorizer \n",
    "#5000 features were used for tfidf vectorizer\n",
    "#r'\\w{1,}'  indicates more than 1 word\n",
    "#ngram_range = (1,3) means 2 and 3 features are used\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word',ngram_range=(1,3), max_features=5000, token_pattern=r'\\w{1,}')\n",
    "\n",
    "q1ngram_trans = tfidf.fit_transform(train['question1'].values)\n",
    "q2ngram_trans = tfidf.fit_transform(train['question2'].values)\n",
    "\n",
    "X = scipy.sparse.hstack((q1ngram_trans,q2ngram_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\Sem_5\\NLP\\Project\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:18:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:18:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, eta=0.3, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=1, subsample=0.8, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "ngram_range Confusion Matrix:\n",
      " [[68497  7853]\n",
      " [17867 27070]]\n",
      "ngram_range Accuracy score: \n",
      " 0.7879409994475912\n",
      "ngram_range Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84     76350\n",
      "           1       0.78      0.60      0.68     44937\n",
      "\n",
      "    accuracy                           0.79    121287\n",
      "   macro avg       0.78      0.75      0.76    121287\n",
      "weighted avg       0.79      0.79      0.78    121287\n",
      "\n",
      "ngram_range F1 Score:\n",
      "  0.6779363886801902\n",
      "Code run-time:  0:10:39.128000\n"
     ]
    }
   ],
   "source": [
    "# Xg Boost classifier for ngram_range=(1,3) level vectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "classifier3 = XGBClassifier(\n",
    "    max_depth=50, n_estimators=80, \n",
    "    learning_rate=0.1, colsample_bytree=.7, \n",
    "    gamma=0, reg_alpha=4, \n",
    "    objective='binary:logistic', eta=0.3, \n",
    "    silent=1, subsample=0.8\n",
    ")\n",
    "\n",
    "#fitting the model with traing data\n",
    "print(classifier3.fit(X_train, y_train))\n",
    "\n",
    "#predicting the test data\n",
    "prediction_tfidf = classifier3.predict(X_test)\n",
    "\n",
    "#Performance evaluation\n",
    "print(\"ngram_range Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\n",
    "print(\"ngram_range Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\n",
    "print(\"ngram_range Classification report:\\n\", classification_report(y_test, prediction_tfidf))\n",
    "print(\"ngram_range F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n",
    "\n",
    "et = datetime.datetime.now()\n",
    "print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF ngram level vectorizer \n",
    "#5000 features were used for tfidf vectorizer\n",
    "#r'\\w{1,}'  indicates more than 1 word\n",
    "#ngram_range = (2,3) means 2 and 3 features are used\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word',ngram_range=(2,3), max_features=5000, token_pattern=r'\\w{1,}')\n",
    "\n",
    "q1ngram_trans = tfidf.fit_transform(train['question1'].values)\n",
    "q2ngram_trans = tfidf.fit_transform(train['question2'].values)\n",
    "\n",
    "X = scipy.sparse.hstack((q1ngram_trans,q2ngram_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\Sem_5\\NLP\\Project\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:31:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, eta=0.3, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=1, subsample=0.8, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "ngram_range(2,3) Confusion Matrix:\n",
      " [[70401  5949]\n",
      " [25777 19160]]\n",
      "ngram_range(2,3) Accuracy score: \n",
      " 0.7384220897540544\n",
      "ngram_range(2,3) Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.82     76350\n",
      "           1       0.76      0.43      0.55     44937\n",
      "\n",
      "    accuracy                           0.74    121287\n",
      "   macro avg       0.75      0.67      0.68    121287\n",
      "weighted avg       0.74      0.74      0.72    121287\n",
      "\n",
      "ngram_range(2,3) F1 Score:\n",
      "  0.5470690688975816\n",
      "Code run-time:  0:04:35.810197\n"
     ]
    }
   ],
   "source": [
    "# Xg Boost classifier for ngram_range=(1,3) level vectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "classifier4 = XGBClassifier(\n",
    "    max_depth=50, n_estimators=80, \n",
    "    learning_rate=0.1, colsample_bytree=.7, \n",
    "    gamma=0, reg_alpha=4, \n",
    "    objective='binary:logistic', eta=0.3, \n",
    "    silent=1, subsample=0.8\n",
    ")\n",
    "\n",
    "#fitting the model with traing data\n",
    "print(classifier4.fit(X_train, y_train))\n",
    "\n",
    "#predicting the test data\n",
    "prediction_tfidf = classifier4.predict(X_test)\n",
    "\n",
    "#Performance evaluation\n",
    "print(\"ngram_range(2,3) Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\n",
    "print(\"ngram_range(2,3) Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\n",
    "print(\"ngram_range(2,3) Classification report:\\n\", classification_report(y_test, prediction_tfidf))\n",
    "print(\"ngram_range(2,3) F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n",
    "\n",
    "et = datetime.datetime.now()\n",
    "print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (character level)+ XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF ngram level vectorizer \n",
    "#5000 features were used for tfidf vectorizer\n",
    "#r'\\w{1,}'  indicates more than 1 word\n",
    "#ngram_range = (1,3) means 2 and 3 features are used\n",
    "#char level analyzer is used \n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='char',ngram_range=(1,3), max_features=5000, token_pattern=r'\\w{1,}')\n",
    "\n",
    "q1char_trans = tfidf.fit_transform(train['question1'].values)\n",
    "q2char_trans = tfidf.fit_transform(train['question2'].values)\n",
    "\n",
    "X = scipy.sparse.hstack((q1char_trans,q2char_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\Sem_5\\NLP\\Project\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:53:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:54:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, eta=0.3, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=1, subsample=0.8, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "char level Confusion Matrix:\n",
      " [[69812  6538]\n",
      " [14864 30073]]\n",
      "char level Accuracy score: \n",
      " 0.8235425066165376\n",
      "char level Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.87     76350\n",
      "           1       0.82      0.67      0.74     44937\n",
      "\n",
      "    accuracy                           0.82    121287\n",
      "   macro avg       0.82      0.79      0.80    121287\n",
      "weighted avg       0.82      0.82      0.82    121287\n",
      "\n",
      "char level F1 Score:\n",
      "  0.7375533428165008\n",
      "Code run-time:  1:17:31.533615\n"
     ]
    }
   ],
   "source": [
    "# Xg Boost classifier for char level vectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "classifier5 = XGBClassifier(\n",
    "    max_depth=50, n_estimators=80, \n",
    "    learning_rate=0.1, colsample_bytree=.7, \n",
    "    gamma=0, reg_alpha=4, \n",
    "    objective='binary:logistic', eta=0.3, \n",
    "    silent=1, subsample=0.8\n",
    ")\n",
    "\n",
    "#fitting the model with traing data\n",
    "print(classifier5.fit(X_train, y_train))\n",
    "\n",
    "#predicting the test data\n",
    "prediction_tfidf = classifier5.predict(X_test)\n",
    "\n",
    "#Performance evaluation\n",
    "print(\"char level Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\n",
    "print(\"char level Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\n",
    "print(\"char level Classification report:\\n\", classification_report(y_test, prediction_tfidf))\n",
    "print(\"char level F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n",
    "\n",
    "et = datetime.datetime.now()\n",
    "print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(classifier1, open(\"./models/analysis1/classfier1.dat\", \"wb\"))\n",
    "pickle.dump(classifier2, open(\"./models/analysis1/classfier2.dat\", \"wb\"))\n",
    "pickle.dump(classifier3, open(\"./models/analysis1/classfier3.dat\", \"wb\"))\n",
    "pickle.dump(classifier4, open(\"./models/analysis1/classfier4.dat\", \"wb\"))\n",
    "pickle.dump(classifier5, open(\"./models/analysis1/classfier5.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48cb8e7cdb11ca655815c41ac34bed6720108475fb3aac9b379c032798fb60bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

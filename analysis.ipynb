{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import scipy\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    f1_score,\n",
    "    accuracy_score, \n",
    "    recall_score, \n",
    "    precision_score\n",
    ")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = './dataset/'\n",
    "# train = pd.read_csv(f'{BASE_DIR}train_preprocessed.csv')\n",
    "train = pd.read_csv(f'{BASE_DIR}train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255027\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_str(s):\n",
    "    return ' '+s+' '\n",
    "\n",
    "def normalize_text(text):\n",
    "    SPECIAL_TOKENS = {'non-ascii': 'non_ascii_word'}\n",
    "\n",
    "    if pd.isnull(text) or len(text)==0:\n",
    "        return ''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) \n",
    "    text = [word for word in text if word not in punctuation]\n",
    "    text = ''.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what is the story of kohinoor kohinoor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math2324math is divide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  what is the step by step guide to invest in sh...   \n",
       "1   1     3     4     what is the story of kohinoor kohinoor diamond   \n",
       "2   2     5     6  how can i increase the speed of my internet co...   \n",
       "3   3     7     8   why am i mentally very lonely how can i solve it   \n",
       "4   4     9    10  which one dissolve in water quikly sugar salt ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  what is the step by step guide to invest in sh...             0  \n",
       "1  what would happen if the indian government sto...             0  \n",
       "2  how can internet speed be increased by hacking...             0  \n",
       "3  find the remainder when math2324math is divide...             0  \n",
       "4             which fish would survive in salt water             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['question1'] = train['question1'].apply(normalize_text)\n",
    "train['question2'] = train['question2'].apply(normalize_text)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('./processed_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words + XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer(analyzer='word', stop_words='english', token_pattern=r'\\w{1,}')\n",
    "q1_trans = CV.fit_transform(train['question1'].values)\n",
    "q2_trans = CV.fit_transform(train['question2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scipy.sparse.hstack((q1_trans, q2_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((283003, 155581), (121287, 155581))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "classifier1 = pickle.load(open(\"models/analysis1/classfier1.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7490992439420548\n",
      "F1 score: 0.5610512498737865\n",
      "Precision: 0.7973759737597376\n",
      "Recall: 0.4327836749226695\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier1.predict(X_test)\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:29:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:29:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, eta=0.3, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=1, subsample=0.8, tree_method='exact',\n",
      "              use_label_encoder=False, validate_parameters=1, ...)\n",
      "Confusion Matrix:\n",
      " [[71408  4942]\n",
      " [25489 19448]]\n",
      "Accuracy score: \n",
      " 0.7490992439420548\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.94      0.82     76350\n",
      "           1       0.80      0.43      0.56     44937\n",
      "\n",
      "    accuracy                           0.75    121287\n",
      "   macro avg       0.77      0.68      0.69    121287\n",
      "weighted avg       0.76      0.75      0.73    121287\n",
      "\n",
      "F1 Score:\n",
      "  0.5610512498737865\n",
      "Code run-time:  0:01:42.107291\n"
     ]
    }
   ],
   "source": [
    "# st = datetime.datetime.now()\n",
    "\n",
    "# classifier1 = XGBClassifier(\n",
    "#     max_depth=50, \n",
    "#     n_estimators=80, ## number of boosting rounds\n",
    "#     learning_rate=0.1, \n",
    "#     colsample_bytree=.7, ## Subsample ratio of columns when constructing each tree.\n",
    "#     gamma=0, ##  Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "#     reg_alpha=4, ##  L1 regularization term on weights\n",
    "#     objective='binary:logistic', \n",
    "#     subsample=0.8, ## Subsample ratio of the training instance.\n",
    "# )\n",
    "\n",
    "# print(classifier1.fit(X_train, y_train))\n",
    "# y_pred = classifier1.predict(X_test)\n",
    "\n",
    "# print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Accuracy score: \\n\", accuracy_score(y_test, y_pred))\n",
    "# print(\"F1 Score:\\n \",f1_score(y_test, y_pred))\n",
    "\n",
    "# et = datetime.datetime.now()\n",
    "# print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF (unigram) + XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 features were used for tfidf vectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', max_features=5000, token_pattern=r'\\w{1,}')\n",
    "\n",
    "q1word_trans = tfidf.fit_transform(train['question1'].values)\n",
    "q2word_trans = tfidf.fit_transform(train['question2'].values)\n",
    "\n",
    "X = scipy.sparse.hstack((q1word_trans,q2word_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "classifier2 =pickle.load(open(\"./models/analysis1/classfier2.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.787679138933861\n",
      "F1 score: 0.6758887401457683\n",
      "Precision: 0.7781231887876073\n",
      "Recall: 0.5973989199247588\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier2.predict(X_test)\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:31:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:31:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, eta=0.3, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=1, subsample=0.8, tree_method='exact',\n",
      "              use_label_encoder=False, validate_parameters=1, ...)\n",
      "Confusion Matrix:\n",
      " [[75553  8422]\n",
      " [19905 29536]]\n",
      "Accuracy score: \n",
      " 0.787679138933861\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84     83975\n",
      "           1       0.78      0.60      0.68     49441\n",
      "\n",
      "    accuracy                           0.79    133416\n",
      "   macro avg       0.78      0.75      0.76    133416\n",
      "weighted avg       0.79      0.79      0.78    133416\n",
      "\n",
      "F1 Score:\n",
      "  0.6758887401457683\n",
      "Code run-time:  0:06:44.488301\n"
     ]
    }
   ],
   "source": [
    "# st = datetime.datetime.now()\n",
    "\n",
    "# classifier2 = XGBClassifier(\n",
    "#    max_depth=50, \n",
    "#     n_estimators=80, ## number of boosting rounds\n",
    "#     learning_rate=0.1, \n",
    "#     colsample_bytree=.7, ## Subsample ratio of columns when constructing each tree.\n",
    "#     gamma=0, ##  Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "#     reg_alpha=4, ##  L1 regularization term on weights\n",
    "#     objective='binary:logistic', \n",
    "#     subsample=0.8, ## Subsample ratio of the training instance.\n",
    "# )\n",
    "# print(classifier2.fit(X_train, y_train))\n",
    "# y_pred = classifier2.predict(X_test)\n",
    "\n",
    "# print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Accuracy score: \\n\", accuracy_score(y_test, y_pred))\n",
    "# print(\"F1 Score:\\n \", f1_score(y_test, y_pred))\n",
    "\n",
    "# et = datetime.datetime.now()\n",
    "# print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf (bigram level) + XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',ngram_range=(2,2), max_features=5000, token_pattern=r'\\w{1,}')\n",
    "\n",
    "q1ngram_trans = tfidf.fit_transform(train['question1'].values)\n",
    "q2ngram_trans = tfidf.fit_transform(train['question2'].values)\n",
    "\n",
    "X = scipy.sparse.hstack((q1ngram_trans, q2ngram_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "classifier3 = pickle.load(open(\"./models/analysis1/classfier3.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7403761326440591\n",
      "F1 score: 0.5502535171034779\n",
      "Precision: 0.7681234548209586\n",
      "Recall: 0.4286668001869284\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier3.predict(X_test)\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\Sem_5\\NLP\\Project\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:04:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              subsample=0.8, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "ngram_range (2,2) Confusion Matrix:\n",
      " [[70535  5815]\n",
      " [25674 19263]]\n",
      "ngram_range (2,2) Accuracy score: \n",
      " 0.7403761326440591\n",
      "ngram_range (2,2) Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.82     76350\n",
      "           1       0.77      0.43      0.55     44937\n",
      "\n",
      "    accuracy                           0.74    121287\n",
      "   macro avg       0.75      0.68      0.68    121287\n",
      "weighted avg       0.75      0.74      0.72    121287\n",
      "\n",
      "ngram_range (2,2) F1 Score:\n",
      "  0.5502535171034779\n",
      "Code run-time:  0:03:02.286574\n"
     ]
    }
   ],
   "source": [
    "# st = datetime.datetime.now()\n",
    "\n",
    "# classifier3 = XGBClassifier(\n",
    "#     max_depth=50, \n",
    "#     n_estimators=80, ## number of boosting rounds\n",
    "#     learning_rate=0.1, \n",
    "#     colsample_bytree=.7, ## Subsample ratio of columns when constructing each tree.\n",
    "#     gamma=0, ##  Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "#     reg_alpha=4, ##  L1 regularization term on weights\n",
    "#     objective='binary:logistic', \n",
    "#     subsample=0.8, ## Subsample ratio of the training instance.\n",
    "# )\n",
    "# classifier3.fit(X_train, y_train)\n",
    "# y_pred = classifier3.predict(X_test)\n",
    "\n",
    "# print(\"Bigram Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Bigram Accuracy score: \\n\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Bigram F1 Score:\\n \",f1_score(y_test, y_pred))\n",
    "\n",
    "# et = datetime.datetime.now()\n",
    "# print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdf (trigram) + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',ngram_range=(3,3), max_features=5000, token_pattern=r'\\w{1,}')\n",
    "\n",
    "q1ngram_trans = tfidf.fit_transform(train['question1'].values)\n",
    "q2ngram_trans = tfidf.fit_transform(train['question2'].values)\n",
    "\n",
    "X = scipy.sparse.hstack((q1ngram_trans,q2ngram_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "classifier4 = pickle.load(open(\"./models/analysis1/classfier4.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7148746361934915\n",
      "F1 score: 0.46482404283636136\n",
      "Precision: 0.7630709821655404\n",
      "Recall: 0.3342012150343815\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier4.predict(X_test)\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\Sem_5\\NLP\\Project\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:12:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=50,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=80, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=1,\n",
      "              subsample=0.8, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "Trigram Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.94      0.81     76350\n",
      "           1       0.76      0.33      0.46     44937\n",
      "\n",
      "    accuracy                           0.71    121287\n",
      "   macro avg       0.73      0.64      0.64    121287\n",
      "weighted avg       0.73      0.71      0.68    121287\n",
      "\n",
      "Trigram Accuracy score: \n",
      " 0.7148746361934915\n",
      "Trigram F1 Score:\n",
      "  0.46482404283636136\n",
      "Code run-time:  0:01:28.419524\n"
     ]
    }
   ],
   "source": [
    "# st = datetime.datetime.now()\n",
    "\n",
    "# classifier4 = XGBClassifier(\n",
    "#     max_depth=50, \n",
    "#     n_estimators=80, ## number of boosting rounds\n",
    "#     learning_rate=0.1, \n",
    "#     colsample_bytree=.7, ## Subsample ratio of columns when constructing each tree.\n",
    "#     gamma=0, ##  Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "#     reg_alpha=4, ##  L1 regularization term on weights\n",
    "#     objective='binary:logistic', \n",
    "#     subsample=0.8, ## Subsample ratio of the training instance.\n",
    "# )\n",
    "\n",
    "# print(classifier4.fit(X_train, y_train))\n",
    "# prediction_tfidf = classifier4.predict(X_test)\n",
    "\n",
    "# print(\"Trigram Classification report:\\n\", classification_report(y_test, prediction_tfidf))\n",
    "# print(\"Trigram Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\n",
    "# print(\"Trigram F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n",
    "\n",
    "# et = datetime.datetime.now()\n",
    "# print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (character level)+ XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF ngram level vectorizer \n",
    "#5000 features were used for tfidf vectorizer\n",
    "#r'\\w{1,}'  indicates more than 1 word\n",
    "#ngram_range = (1,3) means 2 and 3 features are used\n",
    "#char level analyzer is used \n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='char',ngram_range=(1,3), max_features=5000, token_pattern=r'\\w{1,}')\n",
    "\n",
    "q1char_trans = tfidf.fit_transform(train['question1'].values)\n",
    "q2char_trans = tfidf.fit_transform(train['question2'].values)\n",
    "\n",
    "X = scipy.sparse.hstack((q1char_trans,q2char_trans))\n",
    "y = train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16148/1218282631.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16148/1899939083.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#fitting the model with traing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#predicting the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Xg Boost classifier for char level vectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "classifier5 = XGBClassifier(\n",
    "    max_depth=50, n_estimators=80, \n",
    "    learning_rate=0.1, colsample_bytree=.7, \n",
    "    gamma=0, reg_alpha=4, \n",
    "    objective='binary:logistic', eta=0.3, \n",
    "    silent=1, subsample=0.8\n",
    ")\n",
    "\n",
    "#fitting the model with traing data\n",
    "print(classifier5.fit(X_train, y_train))\n",
    "\n",
    "#predicting the test data\n",
    "prediction_tfidf = classifier5.predict(X_test)\n",
    "\n",
    "#Performance evaluation\n",
    "print(\"char level Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\n",
    "print(\"char level Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\n",
    "print(\"char level Classification report:\\n\", classification_report(y_test, prediction_tfidf))\n",
    "print(\"char level F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n",
    "\n",
    "et = datetime.datetime.now()\n",
    "print(\"Code run-time: \", et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(classifier1, open(\"./models/analysis1/bow_xg.pkl\", \"wb\")) ## bow\n",
    "pickle.dump(classifier2, open(\"./models/analysis1/unigram_xg.pkl\", \"wb\")) ## unigram\n",
    "pickle.dump(classifier3, open(\"./models/analysis1/bigram_xg.pkl\", \"wb\")) ## bigram\n",
    "pickle.dump(classifier4, open(\"./models/analysis1/trigram_xg.pkl\", \"wb\")) ## trigram\n",
    "# pickle.dump(classifier5, open(\"./models/analysis1/classfier5.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier4, open(\"./models/analysis1/classfier4.dat\", \"wb\")) ## bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48cb8e7cdb11ca655815c41ac34bed6720108475fb3aac9b379c032798fb60bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

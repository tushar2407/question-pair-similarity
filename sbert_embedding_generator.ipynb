{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sbert_embedding_generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axGpbDnkujRx",
        "outputId": "6fe39e54-c278-424f-982a-0fbc4d73d826"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XB6vkMaulEH",
        "outputId": "f1cb3f3b-53d0-4b6a-fa48-5e7e4fb0bcb5"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.12.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwtq5bxvuH1q"
      },
      "source": [
        "##imports\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "from pathlib import Path\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from functools import reduce\n",
        "from math import log\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "import pickle\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "t = TweetTokenizer()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlAN74OCuni1"
      },
      "source": [
        "dir=\"/content/drive/MyDrive/nlp_project/\"\n",
        "os.chdir(dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-ZmgZhIuo2F"
      },
      "source": [
        "df=pd.read_csv(\"train_with_entities.csv\")\n",
        "output=list(df[\"is_duplicate\"])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAqe7LLiy4lq"
      },
      "source": [
        "#instantiating transformer\n",
        "transmodel = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us0CW_5QtBUk"
      },
      "source": [
        "#setting up data\n",
        "df['question1'].fillna('', inplace=True)\n",
        "df['question2'].fillna('', inplace=True)\n",
        "df['question1_entities'].fillna('', inplace=True)\n",
        "df['question2_entities'].fillna('', inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt07yXjiy-gf"
      },
      "source": [
        "text_1=list(df['question1'])\n",
        "text_2=list(df['question2'])\n",
        "nouns_1=df['question1_entities']\n",
        "nouns_2=df['question2_entities']\n",
        "#Sentences are encoded by calling model.encode()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhAEdLgZkbob"
      },
      "source": [
        "##generate both para and noun embeddings\n",
        "\n",
        "# embeddings1 = list(transmodel.encode(text_1))\n",
        "# open_file = open(dir+\"embeddings_nouns_1\", \"wb\")\n",
        "# pickle.dump(embeddings1, open_file)\n",
        "# open_file.close()\n",
        "embeddings1 = list(transmodel.encode(nouns_1))\n",
        "open_file = open(dir+\"embeddings_entities_1\", \"wb\")\n",
        "pickle.dump(embeddings1, open_file)\n",
        "open_file.close()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTWMBCh6ufXh"
      },
      "source": [
        "# embeddings2 = list(transmodel.encode(text_2))\n",
        "# open_file = open(dir+\"embeddings_para_2\", \"wb\")\n",
        "# pickle.dump(embeddings2, open_file)\n",
        "# open_file.close()\n",
        "embeddings2 = list(transmodel.encode(nouns_2))\n",
        "open_file = open(dir+\"embeddings_entities_2\", \"wb\")\n",
        "pickle.dump(embeddings2, open_file)\n",
        "open_file.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piegia7qy_JK"
      },
      "source": [
        "##loading the embeddings\n",
        "open_file = open(\"embeddings_para_1\", \"rb\")\n",
        "embeddings1= pickle.load(open_file)\n",
        "open_file.close()\n",
        "open_file = open(\"embeddings_para_2\", \"rb\")\n",
        "embeddings2 = pickle.load(open_file)\n",
        "open_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_wwbU_s7Apf",
        "outputId": "ce974d15-5daa-4bd0-e6ab-230b8ed3340a"
      },
      "source": [
        "embeddings1=np.array(embeddings1)\n",
        "embeddings2=np.array(embeddings2)\n",
        "\n",
        "print(embeddings1.shape)\n",
        "print(embeddings2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404348, 384)\n",
            "(404348, 384)\n"
          ]
        }
      ]
    }
  ]
}